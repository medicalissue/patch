# GPU ë©”ëª¨ë¦¬ ìµœì í™” ê°€ì´ë“œ

## ë¬¸ì œ: ë©”ëª¨ë¦¬ ëˆ„ì  (Memory Accumulation)

### ì´ì „ êµ¬í˜„ì˜ ë¬¸ì œì 

```python
# âŒ BAD: ëª¨ë“  ì„ë² ë”©ì„ ë¦¬ìŠ¤íŠ¸ì— ëˆ„ì 
train_embeddings = []
for imgs, _ in dataloader:
    embeddings_batch = extract_embeddings(imgs)
    for b in range(embeddings_batch.shape[0]):
        train_embeddings.append(embeddings_batch[b])  # ë©”ëª¨ë¦¬ ëˆ„ì !

# ì´í›„ í•œë²ˆì— í•™ìŠµ
model.train(train_embeddings)  # OOM ë°œìƒ!
```

**ë¬¸ì œì :**
- 1000ê°œ ì´ë¯¸ì§€ Ã— 49 ìœ„ì¹˜ (7Ã—7) Ã— 16 ë ˆì´ì–´ Ã— 128 ì°¨ì› = **~98M elements**
- GPU ë©”ëª¨ë¦¬ì— ëª¨ë“  ë°ì´í„°ë¥¼ ìŒ“ìŒ
- ë°°ì¹˜ë§ˆë‹¤ ë©”ëª¨ë¦¬ê°€ ê³„ì† ëˆ„ì ë˜ì–´ OOM (Out Of Memory) ë°œìƒ

## í•´ê²°: ìŠ¤íŠ¸ë¦¬ë° í•™ìŠµ (Streaming Training)

### ìƒˆë¡œìš´ êµ¬í˜„

```python
# âœ… GOOD: ë°°ì¹˜ë§ˆë‹¤ ì¦‰ì‹œ í•™ìŠµí•˜ê³  ë©”ëª¨ë¦¬ í•´ì œ
for imgs, _ in dataloader:
    embeddings_batch = extract_embeddings(imgs)
    
    # ì¦‰ì‹œ í•™ìŠµ
    loss = model.train_on_batch(embeddings_batch)
    
    # ë©”ëª¨ë¦¬ í•´ì œ
    del embeddings_batch
    torch.cuda.empty_cache()
```

**ì¥ì :**
- âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ë°°ì¹˜ í¬ê¸°ì—ë§Œ ë¹„ë¡€ (ì¼ì •)
- âœ… ì„ë² ë”©ì„ ë¦¬ìŠ¤íŠ¸ì— ëˆ„ì í•˜ì§€ ì•ŠìŒ
- âœ… ë°°ì¹˜ ì²˜ë¦¬ í›„ ì¦‰ì‹œ ë©”ëª¨ë¦¬ í•´ì œ
- âœ… ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ë„ ì²˜ë¦¬ ê°€ëŠ¥

## êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### 1. ModelTrainerì— ìŠ¤íŠ¸ë¦¬ë° ë©”ì„œë“œ ì¶”ê°€

#### `train_streaming()` - Phase 1 í•™ìŠµ

```python
def train_streaming(self, dataloader, extractor, num_epochs=10):
    """ë°°ì¹˜ë§ˆë‹¤ ì¦‰ì‹œ í•™ìŠµ (ë©”ëª¨ë¦¬ ëˆ„ì  ì—†ìŒ)"""
    for epoch in range(num_epochs):
        for imgs, _ in dataloader:
            imgs_gpu = imgs.to(self.device, non_blocking=True)
            
            # ì„ë² ë”© ì¶”ì¶œ
            with torch.no_grad():
                activations = extractor(imgs_gpu)
                embeddings_batch = stack_trajectory(activations)
            
            # ì¦‰ì‹œ í•™ìŠµ
            loss = self.train_on_batch(embeddings_batch)
            
            # CRITICAL: ë©”ëª¨ë¦¬ í•´ì œ
            del imgs_gpu, activations, embeddings_batch
            torch.cuda.empty_cache()
```

#### `adapt_with_lora_streaming()` - Phase 2 LoRA ì ì‘

```python
def adapt_with_lora_streaming(self, dataloader, extractor, lora_cfg, num_epochs=5):
    """LoRA ì ì‘ë„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹"""
    # LoRA ì„¤ì • ë° optimizer ìƒì„±
    self.model, lora_params = apply_lora_to_model(self.model, ...)
    lora_optimizer = torch.optim.Adam(lora_params, ...)
    
    for epoch in range(num_epochs):
        for imgs, _ in dataloader:
            imgs_gpu = imgs.to(self.device, non_blocking=True)
            
            # ì„ë² ë”© ì¶”ì¶œ
            with torch.no_grad():
                activations = extractor(imgs_gpu)
                embeddings_batch = stack_trajectory(activations)
            
            # LoRA í•™ìŠµ
            trajectories = embeddings_batch.reshape(-1, L, D)
            reconstruction, mu, logvar = self.model(trajectories)
            loss = compute_loss(...)
            loss.backward()
            lora_optimizer.step()
            
            # CRITICAL: ë©”ëª¨ë¦¬ í•´ì œ
            del imgs_gpu, activations, embeddings_batch, trajectories
            del reconstruction, loss
            if vae:
                del mu, logvar
            torch.cuda.empty_cache()
```

### 2. test.py ìˆ˜ì •

#### Phase 1 (ëª¨ë¸ í•™ìŠµ)

```python
# âŒ Before: ëª¨ë“  ì„ë² ë”© ìˆ˜ì§‘ í›„ í•™ìŠµ
train_embeddings = []
for imgs, _ in imagenet_loader:
    embeddings = extract_and_stack(imgs)
    train_embeddings.append(embeddings)  # ë©”ëª¨ë¦¬ ëˆ„ì !
model_trainer.train(train_embeddings)  # OOM!

# âœ… After: ìŠ¤íŠ¸ë¦¬ë° í•™ìŠµ
model_trainer.train_streaming(
    imagenet_loader,
    extractor,
    num_epochs=10
)  # ë©”ëª¨ë¦¬ ì¼ì •!
```

#### Phase 2 (LoRA ì ì‘)

```python
# âŒ Before: ë„ë©”ì¸ ì„ë² ë”© ìˆ˜ì§‘ í›„ ì ì‘
domain_embeddings = []
for imgs, _ in domain_loader:
    embeddings = extract_and_stack(imgs)
    domain_embeddings.append(embeddings)  # ë©”ëª¨ë¦¬ ëˆ„ì !
model_trainer.adapt_with_lora(domain_embeddings)

# âœ… After: ìŠ¤íŠ¸ë¦¬ë° ì ì‘
model_trainer.adapt_with_lora_streaming(
    domain_loader,
    extractor,
    lora_cfg=cfg.domain_adaptation.lora,
    num_epochs=5
)  # ë©”ëª¨ë¦¬ ì¼ì •!
```

## ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ

### ì´ì „ ë°©ì‹ (ëˆ„ì )

```
ë°°ì¹˜ 1:  1 GB
ë°°ì¹˜ 2:  2 GB  â¬†ï¸
ë°°ì¹˜ 3:  3 GB  â¬†ï¸â¬†ï¸
ë°°ì¹˜ 4:  4 GB  â¬†ï¸â¬†ï¸â¬†ï¸
...
ë°°ì¹˜ N:  OOM! ğŸ’¥  (ë©”ëª¨ë¦¬ ë¶€ì¡±)
```

### ìƒˆë¡œìš´ ë°©ì‹ (ìŠ¤íŠ¸ë¦¬ë°)

```
ë°°ì¹˜ 1:  1 GB  â†’  í•´ì œ  â†’  0 GB
ë°°ì¹˜ 2:  1 GB  â†’  í•´ì œ  â†’  0 GB
ë°°ì¹˜ 3:  1 GB  â†’  í•´ì œ  â†’  0 GB
ë°°ì¹˜ 4:  1 GB  â†’  í•´ì œ  â†’  0 GB
...
ë°°ì¹˜ N:  1 GB  â†’  í•´ì œ  â†’  0 GB  âœ… (ì¼ì •)
```

## ì¶”ê°€ ë©”ëª¨ë¦¬ ì ˆì•½ íŒ

### 1. Gradient Accumulation ì‚¬ìš©

ë°°ì¹˜ í¬ê¸°ë¥¼ ì¤„ì´ê³  gradient accumulationìœ¼ë¡œ íš¨ê³¼ì ì¸ ë°°ì¹˜ í¬ê¸° ì¦ê°€:

```yaml
# config.yaml
data:
  imagenet:
    batch_size: 64  # 128 â†’ 64ë¡œ ê°ì†Œ
    gradient_accumulation_steps: 2  # íš¨ê³¼ì ìœ¼ë¡œ 128ê³¼ ë™ì¼
```

### 2. Mixed Precision Training

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    loss = model.train_on_batch(embeddings)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 3. ì‘ì€ ëª¨ë¸ ì‚¬ìš©

```yaml
# config.yaml
model:
  type: autoencoder  # transformer ëŒ€ì‹  ì‚¬ìš© (ë” ê°€ë²¼ì›€)
  hidden_dim: 64     # 128 â†’ 64ë¡œ ê°ì†Œ
  latent_dim: 32     # 64 â†’ 32ë¡œ ê°ì†Œ
  num_layers: 1      # 2 â†’ 1ë¡œ ê°ì†Œ
```

### 4. ë‚®ì€ spatial resolution

```yaml
model:
  spatial_resolution: 7  # 14 â†’ 7ë¡œ ê°ì†Œ (49 vs 196 ìœ„ì¹˜)
```

## ì‚¬ìš©ë²•

### ê¸°ë³¸ ì‚¬ìš© (ìë™ìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ ì ìš©)

```bash
python test.py
```

### ë©”ëª¨ë¦¬ê°€ ë¶€ì¡±í•œ ê²½ìš° ì¶”ê°€ ì˜µì…˜

```bash
# ë°°ì¹˜ í¬ê¸° ê°ì†Œ
python test.py data.imagenet.batch_size=32 data.domain.batch_size=16

# ë” ì‘ì€ ëª¨ë¸ ì‚¬ìš©
python test.py model.hidden_dim=64 model.latent_dim=32

# ë‚®ì€ í•´ìƒë„ ì‚¬ìš©
python test.py model.spatial_resolution=7

# ìƒ˜í”Œ ìˆ˜ ê°ì†Œ
python test.py data.imagenet.num_samples=500
```

## ì„±ëŠ¥ ë¹„êµ

| ë°©ì‹ | ë©”ëª¨ë¦¬ ì‚¬ìš© | í•™ìŠµ ì†ë„ | í™•ì¥ì„± |
|------|------------|----------|--------|
| **ì´ì „ (ëˆ„ì )** | O(N) - ì¦ê°€ | ë¹ ë¦„ | ì œí•œì  (ì‘ì€ ë°ì´í„°ì…‹ë§Œ) |
| **ìƒˆë¡œìš´ (ìŠ¤íŠ¸ë¦¬ë°)** | O(1) - ì¼ì • | ì•½ê°„ ëŠë¦¼ | ë¬´ì œí•œ (ëŒ€ìš©ëŸ‰ ê°€ëŠ¥) |

## ê²°ë¡ 

âœ… **ìŠ¤íŠ¸ë¦¬ë° í•™ìŠµ ë°©ì‹**ìœ¼ë¡œ ë³€ê²½í•˜ì—¬:
- ë©”ëª¨ë¦¬ ëˆ„ì  ë¬¸ì œ ì™„ì „ í•´ê²°
- ëŒ€ìš©ëŸ‰ ë°ì´í„°ì…‹ ì²˜ë¦¬ ê°€ëŠ¥
- GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¼ì •í•˜ê²Œ ìœ ì§€
- OOM ì˜¤ë¥˜ ë°©ì§€

ì´ì œ ì•ˆì „í•˜ê²Œ ëŒ€ìš©ëŸ‰ ë°ì´í„°ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ğŸš€
